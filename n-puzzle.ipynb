{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "from random import choice\n",
    "from tqdm.auto import tqdm\n",
    "from collections import namedtuple\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Setup and Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of the puzzle and a named tuple used to represent a move between two positions (`pos1` and `pos2`) are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUZZLE_DIM = 4\n",
    "Action = namedtuple('Action', ['pos1', 'pos2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct positions of each tile are calculated and stored in a dictionary with the number as key and the target coordinates as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_pos():\n",
    "    # row: (n-1) // PUZZLE_DIM, column: n - PUZZLE_DIM * row - 1\n",
    "    correct_pos = {n : ((n-1) // PUZZLE_DIM, n - PUZZLE_DIM * ((n-1) // PUZZLE_DIM) - 1) for n in range(1, PUZZLE_DIM**2)}\n",
    "    correct_pos[0] = (PUZZLE_DIM-1, PUZZLE_DIM-1)\n",
    "    return correct_pos\n",
    "\n",
    "CORRECT_POS = get_correct_pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `available_actions` function returns a list of all possible moves (`Action`) that can be made from the current puzzle state. The `do_action` function takes the current puzzle state and an action and returns the new state obtained after the action is performed. It swaps the empty tile with the tile specified by the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_actions(state: np.ndarray) -> list['Action']:\n",
    "    x_start, y_start = [int(_[0]) for _ in np.where(state == 0)]\n",
    "    actions = list()\n",
    "    if x_start > 0:\n",
    "        actions.append(Action((x_start, y_start), (x_start-1, y_start)))\n",
    "    if x_start < PUZZLE_DIM - 1:\n",
    "        actions.append(Action((x_start, y_start), (x_start+1, y_start)))\n",
    "    if y_start > 0:\n",
    "        actions.append(Action((x_start, y_start), (x_start, y_start-1)))\n",
    "    if y_start < PUZZLE_DIM - 1:\n",
    "        actions.append(Action((x_start, y_start), (x_start, y_start+1)))\n",
    "    return actions\n",
    "\n",
    "def do_action(state: np.ndarray, action: 'Action') -> np.ndarray:\n",
    "    new_state = state.copy()\n",
    "    new_state[action.pos1], new_state[action.pos2] = new_state[action.pos2], new_state[action.pos1]\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different heuristics are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(state: np.ndarray) -> int:\n",
    "    wrong_positions = 0\n",
    "    for n in range(1, PUZZLE_DIM**2):\n",
    "        x_current, y_current = [int(_[0]) for _ in np.where(state == n)]\n",
    "        if x_current != CORRECT_POS[n][0] or y_current != CORRECT_POS[n][1]:\n",
    "            wrong_positions += 1\n",
    "    return wrong_positions\n",
    "\n",
    "def manhattan_distance(state: np.ndarray) -> int:\n",
    "    total_distance = 0\n",
    "    for n in range(1, PUZZLE_DIM**2):\n",
    "        x_current, y_current = [int(_[0]) for _ in np.where(state == n)]\n",
    "        total_distance += abs(x_current - CORRECT_POS[n][0]) + abs(y_current - CORRECT_POS[n][1])\n",
    "    return total_distance\n",
    "\n",
    "def n_linear_conflicts(state: np.ndarray) -> int:\n",
    "    conflicts = 0\n",
    "\n",
    "    # row conflicts\n",
    "    for r in range(PUZZLE_DIM):\n",
    "        for c in range(PUZZLE_DIM):\n",
    "            for k in range(c+1, PUZZLE_DIM):\n",
    "                if(state[r][c] and state[r][k] and CORRECT_POS[state[r][c]][0] == r and \n",
    "                CORRECT_POS[state[r][k]][0] == r and CORRECT_POS[state[r][c]][1] > CORRECT_POS[state[r][k]][1]):\n",
    "                    conflicts += 1\n",
    "    # column conflicts\n",
    "    for c in range(PUZZLE_DIM):\n",
    "        for r in range(PUZZLE_DIM):\n",
    "            for l in range(r+1, PUZZLE_DIM):\n",
    "                if(state[r][c] and state[l][c] and CORRECT_POS[state[r][c]][1] == c and \n",
    "                CORRECT_POS[state[l][c]][1] == c and CORRECT_POS[state[r][c]][0] > CORRECT_POS[state[l][c]][0]):\n",
    "                    conflicts += 1\n",
    "\n",
    "    return conflicts\n",
    "\n",
    "def linear_conflicts_distance(state: np.ndarray) -> int:\n",
    "    return manhattan_distance(state) + 2*n_linear_conflicts(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A precise heuristic is chosen. If you want to use a different heuristic than the one proposed, simply modify the following line of code. It is not recommended to use the Hamming distance for problems larger than 8-puzzle. It does not converge in a feasible amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristic = linear_conflicts_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal state and initial state are defined. The `initial_state` is obtained applying a sequence of random actions to shuffle the goal state. The `RANDOMIZE_STEPS` constant defines the number of random moves to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOMIZE_STEPS = 1_000\n",
    "GOAL_STATE = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "print(f\"GOAL STATE\\n{GOAL_STATE}\")\n",
    "\n",
    "initial_state = GOAL_STATE.copy()\n",
    "for step in range(RANDOMIZE_STEPS): # use tqdm\n",
    "    initial_state = do_action(initial_state, choice(available_actions(initial_state)))\n",
    "    \n",
    "h_distance = heuristic(initial_state)\n",
    "print(f\"\\nINITIAL STATE\\n{initial_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state: np.ndarray, parent, g: int, h: int) -> None:\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.g = g\n",
    "        self.h = h\n",
    "\n",
    "    def __lt__(self, next):\n",
    "        return self.g + self.h < next.g + next.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A* algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The A* (pronounced as \"A star\") algorithm is used to find the **shortest path** between the starting node (the initial configuration of the puzzle) and the goal node (the puzzle solved with all the tiles in the correct positions). It uses a heuristic function to help decide which path to follow next. \n",
    "\n",
    "> **Run it only for 8-puzzle and 15-puzzle**. It does not converge in reasonable time for larger puzzle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontier = []\n",
    "explored = set()\n",
    "n_actions = 0\n",
    "heapq.heappush(frontier, Node(initial_state, None, 0, h_distance))\n",
    "\n",
    "heuristics_history = []\n",
    "\n",
    "while frontier:\n",
    "    n_actions += 1\n",
    "    current_node = heapq.heappop(frontier)\n",
    "    heuristics_history.append(current_node.h)\n",
    "\n",
    "    # puzzle solved\n",
    "    if np.array_equal(current_node.state, GOAL_STATE):\n",
    "        path = []\n",
    "        node = current_node\n",
    "        while node.parent != None:\n",
    "            path.append(node.state)\n",
    "            node = node.parent\n",
    "        path.append(initial_state)\n",
    "        print(path[::-1])\n",
    "        break\n",
    "\n",
    "    # Explore neighbors of the current state if the puzzle is not solved yet\n",
    "    for action in available_actions(current_node.state):\n",
    "        new_state = do_action(current_node.state, action)\n",
    "        if tuple(new_state.flatten()) not in explored:\n",
    "            explored.add(tuple(new_state.flatten()))\n",
    "            heuristic_distance = heuristic(new_state)\n",
    "            heapq.heappush(frontier, Node(new_state, current_node, current_node.g + 1, heuristic_distance))\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nSolve {PUZZLE_DIM*PUZZLE_DIM-1}-puzzle in {len(path)} moves\")\n",
    "print(f\"{n_actions} evaluated actions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic Value Evolution Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(heuristics_history, linewidth = 0.5)\n",
    "plt.xlabel(\"No. steps\")\n",
    "plt.ylabel(\"Value of heuristic distance\")\n",
    "plt.title(\"Heuristic function evolution\")\n",
    "\n",
    "plt.savefig(\"plots/15-puzzle_heuristic.png\", dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal Tree and Problem Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of solving one complex problem (go from an initial configuration to an end configuration), it may be convenient to **decompose** it in a certain number of smaller and easier sub-problems. The key idea is to move each tile in the correct position, one at a time. This step is repeated until all the tiles are in their correct positions. *e.g.* The first subgoal consists of placing tile number 1 in its correct position (0, 0). The second subgoal consists of placing tile number 2 in its correct position (0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions_total = 0\n",
    "\n",
    "def solve_subgoal(initial_state: np.ndarray, target_number: int) -> tuple[np.ndarray, int]:\n",
    "    target_position = CORRECT_POS[target_number]\n",
    "    \n",
    "    frontier = []\n",
    "    explored = set()\n",
    "    heapq.heappush(frontier, Node(initial_state, None, 0, heuristic(initial_state)))\n",
    "\n",
    "    n_actions_dec = 0\n",
    "    while frontier:\n",
    "        n_actions_dec += 1\n",
    "        current_node = heapq.heappop(frontier)\n",
    "\n",
    "        # If the target number is already in the correct position\n",
    "        if tuple(np.argwhere(current_node.state == target_number)[0]) == target_position:\n",
    "            return (current_node.state, n_actions_dec)\n",
    "\n",
    "        # Explore neighbors\n",
    "        for action in available_actions(current_node.state):\n",
    "            new_state = do_action(current_node.state, action)\n",
    "            if tuple(new_state.flatten()) not in explored:\n",
    "                explored.add(tuple(new_state.flatten()))\n",
    "                heapq.heappush(frontier, Node(new_state, current_node, current_node.g + 1, heuristic(new_state)))\n",
    "\n",
    "    return (initial_state, n_actions_dec) # if impossible to solve\n",
    "\n",
    "def solve_n_puzzle(initial_state: np.ndarray) -> tuple[list[np.ndarray], int]:\n",
    "    current_state = initial_state\n",
    "    n_actions_total = 0\n",
    "    steps = []\n",
    "\n",
    "    # Solve small sub-problem for each number\n",
    "    for target_number in range(1, PUZZLE_DIM**2): \n",
    "        current_state, n_actions_dec = solve_subgoal(current_state, target_number)\n",
    "        n_actions_total += n_actions_dec\n",
    "        steps.append(current_state)\n",
    "\n",
    "    return (steps, n_actions_total)\n",
    "\n",
    "# Print results\n",
    "steps, n_actions_total = solve_n_puzzle(initial_state)\n",
    "print(f\"\\nSolve {PUZZLE_DIM*PUZZLE_DIM-1}-puzzle in {len(steps)} moves\")\n",
    "print(f\"{n_actions_total} evaluated actions\")\n",
    "print(steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CI2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
